# Deploying Kubernetes using Ansible Playbooks.

With your VMs now running and connected to the Network, its time to prepare our Virtual Machines for our Kubernetes deployment. Kubernetes has a few Host OS configurations which we will take care of in this section, notably disabling SWAP, some firewall exceptions, and installing our Container Runtime to be used by the Container Runtime Interface (CRI) later. 

Instead of doing all of this "foundational" work by running a few to many commands directly on the shell of all three nodes, we will instead leverage Ansible (a open-source IaC tool) to do the configuration for us using "playbooks". Luckily, in this repo under the 'ansible' directory you will find two pre-written playbooks (masternode-pb.yaml, workernode-pb.yaml) which will save you writing these yourself. 

Before we can run these Playbooks however, we have to install Ansible, which we will do via Python3! Lets dive in.

## Installing Ansible

Thankfully our minimal Rocky install has Python 3.12 install by default, all that is missing `pip` (pythons package manager) and an explict 'ansible' user to run our playbooks. For convenience, run the `autoAnisible.py` script located under "ansible/autoAnsible.py" in the repo to configure the Ansible "control node" and "worker-nodes".

### Using the `autoAnsible.py` Script
Fetch the Installer from the public repo
```
curl https://raw.githubusercontent.com/ToyoLandi/teleport-concept/refs/heads/main/ansible/autoAnsible.py -o autoAnsible.py
```

Run the Installer with python3 using the applicable '--control-node' or '--worker-node' arguments. 
```
# use the '--control-node' argument for the "challenger-master" node
sudo python3 autoAnsible.py --control-node

# use the '--worker-node' argument for the "challenger-worker-1" and "challenger-worker-2" nodes
sudo python3 autoAnsible.py --worker-node
```

Share the public SSH Key generated by `autoAnsible` from your "control-node"s, 'ansible' users terminal to our "worker-nodes", and to the "control-node" itself after all nodes have been staged - replacing the IPs in brackets with your IPs in use. 
```
su ansible
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@[10.99.0.10]
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@[10.99.0.11]
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@[10.99.0.12]
```
>  We copy the pubkey to control-node to safely update the "known_host" file, allowing the node to accept SSH connectons using its private IP as Ansible will leverage based on our "hosts.yaml" inventory. 

Test our Ansible connections using the ansible user.
```
ansible -i ~/ansible/hosts.yaml all -m ping
```

## Installing Kubernetes with `kubeadm` using Ansible

We should now have a 'ansible' non-root user, which can connect to all three nodes - our soon to be k8s control-plane and two worker nodes - using a custom certificate. Now we can use Ansible Playbooks to quickly...

- Stand-up our `kube` root user 
- Disable Swap
- Install containerd + repo requirements
- Configure Host Networking for containerd
- Install kubeadm, kubelet, and kubectl + repo requirements 

1. From the 'ansible' user terminal, `curl` the "create-kubeUser.yaml" file from this repos "ansible" directory, and modify the user details to match your current root-user (ex. cspears) using `vi` or `nano`

```
curl 
```
Once the playbook aligns with your deployment, run the following ansible-playbook - which will prompt for the password of your admin account you modified on line 2 thanks to the `-K` [argument](https://docs.ansible.com/projects/ansible/latest/playbook_guide/playbooks_privilege_escalation.html#using-become)

```
ansible-playbook -i ~/ansible/hosts.yaml create-kubeuser.yaml -K
```