# Deploying Kubernetes using Ansible Playbooks.

With your VMs now running and connected to the Network, its time to prepare our Virtual Machines for our Kubernetes deployment. 

Instead of doing all of this "foundational" work by running a few to many commands directly on the shell of all three nodes, we will instead leverage Ansible (a open-source IaC tool) to do the configuration for us using "playbooks". Luckily, in this repo under the 'ansible' directory you will find 'install-k8s.yaml' which is our playbook for our kubeadm k8s deployment. 

Before we can run these Playbooks however, we have to install Ansible.

## Installing Ansible

Thankfully our minimal Rocky install has Python 3.12 install by default, all that is missing `pip` (pythons package manager) and an explict 'ansible' user to run our playbooks. For convenience, run the `autoAnisible.py` script located under "ansible/autoAnsible.py" in the repo to configure the Ansible "control-node" and "worker-nodes".

### Using the `autoAnsible.py` Script

Open three terminals and SSH into each new VM for the first time from your local machine - changing the user and IPs to match your deployment.
```
ssh cspears@10.99.0.10

ssh cspears@10.99.0.11

ssh cspears@10.99.0.12
```

Fetch the Installer from the public repo
```
curl https://raw.githubusercontent.com/ToyoLandi/teleport-concept/refs/heads/main/ansible/autoAnsible.py -o autoAnsible.py
```

Run the Installer with python3 using the applicable '--control-node' or '--worker-node' arguments. 
```
# use the '--control-node' argument for the "challenger-master" node
sudo python3 autoAnsible.py --control-node

# use the '--worker-node' argument for the "challenger-worker-1" and "challenger-worker-2" nodes
sudo python3 autoAnsible.py --worker-node
```

From the 'ansible' users terminal of your control-node, share the public SSH Key generated by `autoAnsible` to our worker-nodes, and to the "control-node" itself after all nodes have been staged - **replacing the IPs with your IPs for your environment**.
```
su ansible
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@10.99.0.10
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@10.99.0.11
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@10.99.0.12
```
>  We copy the pubkey to control-node to safely update its "known_host" file for the 'ansible' user, which allows us to run Ansible playbooks on the control-node aswell

Test our Ansible connections using from the 'ansible' user terminal.
```
ansible -i ~/ansible/hosts.yaml all -m ping
```

## Installing `kubeadm` using Ansible

We should now have a 'ansible' non-root user, which can connect to all three nodes - our soon to be k8s control-plane and two worker nodes, using a custom certificate. Now we can use Ansible Playbooks to quickly...

- Disable Swap
- Install containerd + repo requirements
- Configure Host Networking for containerd
- Install kubeadm, kubelet, and kubectl + repo requirements 

1. `curl` the "install-k8s.yaml" playbook file from this repos "ansible" directory, 
```
curl https://raw.githubusercontent.com/ToyoLandi/teleport-concept/refs/heads/main/ansible/install-k8s.yaml -o ~/ansible/install-k8s.yaml 
```

2. Using `vi` or `nano` , Modify the user details (line 6) to match your current root-user (ex. cspears).
```
vi ~/ansible/install-k8s.yaml 
```

3. Begin our kubernetes deployment by running the following ansible-playbook command. Thanks to the `-K` [argument](https://docs.ansible.com/projects/ansible/latest/playbook_guide/playbooks_privilege_escalation.html#using-become) you will be prompted for the `BECOME password`, aka the password of your root account, without needing to expose this in your playbook or command history. 
```
ansible-playbook -i ~/ansible/hosts.yaml ~/ansible/install-k8s.yaml -K
```