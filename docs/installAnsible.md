# Deploying Kubernetes using Ansible Playbooks.

With your VMs now running and connected to the Network, its time to prepare our Virtual Machines for our Kubernetes deployment. Instead of doing all of this "foundational" work by running a few to many commands directly on the shell of all three nodes, we will instead leverage Ansible (a open-source IaC tool) to do the configuration for us using "playbooks". Luckily, in this repo under the 'ansible' directory you will find 'install-k8s.yaml' which is our playbook for our kubeadm k8s deployment. 

Before we can run these Playbooks however, we have to install Ansible.

## Installing Ansible

Thankfully our minimal Rocky install has Python 3.12 install by default, all that is missing `pip` (pythons package manager) and an explict 'ansible' user to run our playbooks. For convenience, run the `autoAnisible.py` script located under "ansible/autoAnsible.py" in the repo to configure the Ansible "control-node" and "worker-nodes".

### Using the `autoAnsible.py` Script

Open three terminals and SSH into each new VM for the first time from your local machine - changing the user and IPs to match your deployment.
```
ssh cspears@10.99.0.10
ssh cspears@10.99.0.11
ssh cspears@10.99.0.12
```

Fetch the Installer from the public repo
```
curl https://raw.githubusercontent.com/ToyoLandi/teleport-concept/refs/heads/main/ansible/autoAnsible.py -o autoAnsible.py
```

Run the Installer with python3 using the applicable '--control-node' or '--worker-node' arguments. 
```
# use the '--control-node' argument for the "challenger-master" node
sudo python3 autoAnsible.py --control-node

# use the '--worker-node' argument for the "challenger-worker-1" and "challenger-worker-2" nodes
sudo python3 autoAnsible.py --worker-node
```

From the 'ansible' users terminal of your control-node, share the public SSH Key generated by `autoAnsible` to our "worker-nodes" + the "control-node" itself after all three have been staged - **replacing the IPs with your IPs for your environment**.
```
su ansible
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@10.99.0.10
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@10.99.0.11
ssh-copy-id -i ~/.ssh/challenger-master_rsa.pub ansible@10.99.0.12
```
>  We copy the pubkey to control-node to safely update its "known_host" file for the 'ansible' user, which allows us to run Ansible playbooks on the control-node aswell

Finally, test our Ansible connections using from the 'ansible' user terminal.
```
ansible -i ~/ansible/hosts.yaml all -m ping
```