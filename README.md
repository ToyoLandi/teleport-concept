# README

The repo is a guide to gracefully deploy a proof-of-concept, 3 node Kubernetes (aka k8s) cluster using `kubeadm` deployed via `ansible` playbooks, and securely deploy a prototype `nginx` Web Server + `cert-manager` using `helm` within the freshly created cluster. 

Included in this repo is a custom python3 script to easily setup Ansible on all three nodes named [autoAnsible.py](https://github.com/ToyoLandi/teleport-concept/blob/main/ansible/autoAnsible.py) as well as custom Ansible playbooks to install and remove Kubernetes, so you dont have to write these yourself.

### This Guide Assumes...
- You have the ability to create three Virtual Machines (aka VMs) in any Hypervisor of your choice, such as VirtualBox, ProxMox or ESXi.
- You have a total of 8 GB of RAM, 40 GB of disk (preferably SSD), and 6 virtual cores available to distribute across the VM's. 
- All three Virtual Machines can reside in the same subnet, have a functional DNS server, can reach the internet, and have a virtual NIC configuration capable of providing a unique MAC address based on the [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address) requirements.
- You have Four IP addresses available for static assignment -OR- the ability to configure DHCP reservations within the VM's destined subnet, as well as a small range of IP address for our Load Balancer pool
- You CANNOT have your cluster facing the internet (no public IP).
- You CANNOT configure DNS A or AAAA records.
- You DO NOT have an existing Certificate Authority available to use.

### How To Use This Repo...
There are several sections to this guide intended to be followed in order, all documented under the "docs" directory to keep the README here more concise. If something isnt making sense, check the previous page in case you overlooked a step or note. Refer to the Table of Contents below for an ordered list of building this solution from scratch.

## Table of Contents
1. [Standing Up our VMs with Rocky Linux 10.1](https://github.com/ToyoLandi/teleport-concept/blob/main/docs/install-vm.md)
2. [Getting Started With Ansible](https://github.com/ToyoLandi/teleport-concept/blob/main/docs/install-ansible.md)
3. [Installing Kubernetes using Ansible + Kubeadm](https://github.com/ToyoLandi/teleport-concept/blob/main/docs/install-k8s.md)
4. [Configuring a User with Kubernetes RBAC](https://github.com/ToyoLandi/teleport-concept/blob/main/docs/k8s-RBAC.md)
5. [Using Helm to deploy our Nginx Website w/ Cert-Manager](https://github.com/ToyoLandi/teleport-concept/blob/main/docs/install-nginx.md)


## ---> Design Overview <---
By the end of this guide we will have...

- Deployed Ansible w/ a custom script ([autoAnsible.py](https://github.com/ToyoLandi/teleport-concept/blob/main/ansible/autoAnsible.py)) + using SSH Certificates to securely handle node->node Authentication.
- An easily editable Ansible inventory file to adapt the deployment to any LAN in a single-place.
- Deployed a Kubernetes Cluster with Two Worker Nodes and a Control-plane using `kubeadm` via Ansible Playbooks,
- Our Cluster will be configured...
    1. Using `containerd` as our Container Runtime Interface (CRI)
    2. ensuring containerd/runc will leverage `systemd` to properly track consumed CPU/Mem Resources across the Cluster & locally on the Kernel, preventing resource over-allocation.
    3. Using `flannel` as our Container Network Interface
    4. Using `kube-vip` and `kube-vip-cloud-controller` to dynamically Load Balance traffic across our Worker Nodes using ARP. Allowing users to simply declare `service.type.LoadBalancer` to expose their Apps to the LAN.
- `cert-manager` deployed via `helm` in its own namespace so it cannot be tampered with by normal users. 
- A normal user authenticating to `kubeapi` using Certificates signed by the Kubernetes CA generated by `kubeadm`, with minimum role permissions, constrainted to the `webservers` namespace.
- A primitive `nginx` StaticSite deployed via `helm` by our normal user, configured to...
    1. Pull the web-contents via `git` (no PVC needed)
    2. Use a certificate secret generated via `cert-manager` for TLS (HTTPS) by `cert-manager.io,issuers` defined explictly in the `webservers` namespace should other certs be needed in the future for this namespace. 
    3. run with **two** replica's so we can host our website on each worker to maintain our service should one node go down.


Intentional Tradeoff's we have taken in this deployment...

- *If* we wished to have multiple control-nodes in HA, we could easily add the `--controlplane` arg to our `init-k8s` and point the `--interface` to the "kube_vip" IP in our playbook to support this kind of deployment. This was omited to limit the number of VM's in our demo. 
- `firewalld` has been disabled, opposed to explicitly poking holes in our firewall policy to reduce network configuration complexity for the demo.
- `flannel` was chosing for simplicity, but other solutions like Calico offer better security (with additional config) 
- We have NOT deployed a reverse-proxy solution like [Traefik](https://github.com/traefik/traefik) to keep our demo concise.
- We are using Self-Signed certs instead of more genuine CA like [Let's Encrypt](https://letsencrypt.org/) even though `cert-manager` supports this easily with a different `cert-manager.io.Issuer` config, to remove the need for public IP's and a registerd domains ($), and to save you needing to stand up your own Certificate Authority. This is also why we use the automatic self-signed certs for the kubernetes deployment itself.
- We are using the default `cluster.local` domain for similar reasons + saving you the need a deploy a DNS service to manager your records (though this would be possible with an elaborate /etc/hosts file)
- We have NOT scripted the Kubeconfig certificate & config file make steps to clearly explain the process. 
- We ask you to `curl` the individual files in the deployment steps to prevent a "file-bomb" for the three users shells we leverage in this solution. 
- we have the `init-k8s` and `stage-k8s` playbooks as seperate steps to give the admin more control between iterations, allowing you to remove or reinstall the cluster without re-staging.